{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pickle.load(open('datasets/user_anime_ratings_mapping.pkl', 'rb'))\n",
    "mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_anime = (108711, 6668)\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = pickle.load(open('user_embed_pytorch_nn_epoch4_embedding_fix_10.23-10.21.pkl', 'rb'))\n",
    "user_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition.pca import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(user_embeddings.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_embeddings = pickle.load(open('anime_embed_pytorch_nn_epoch4_embedding_fix_10.23-10.21.pkl', 'rb'))\n",
    "anime_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_grouped_rating_files = [f for f in glob('datasets/user_grouped_anime_ratings.gz')]\n",
    "user_grouped_rating_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def input_anime_embeddings(record, mapping, anime_embeddings):\n",
    "    num_records = len(record['anime_id'])\n",
    "    batch_anime_idx = np.array([mapping['anime2idx'][x] for x in record['anime_id']], dtype=np.int32)\n",
    "    batch_anime_rating = np.array(record['my_score'], dtype=np.int16)\n",
    "    num_anime_watched = len(batch_anime_idx)\n",
    "    sum_rating = np.sum(batch_anime_rating)\n",
    "    sum_rating = 1 if sum_rating == 0 else sum_rating\n",
    "    sum_neg_rating = np.sum(10 - batch_anime_rating)\n",
    "    sum_neg_rating = 1 if sum_neg_rating == 0 else sum_neg_rating\n",
    "    anime_sum = np.sum(\n",
    "        anime_embeddings[batch_anime_idx] * batch_anime_rating.reshape(-1, 1), axis=0\n",
    "    ).astype(np.float32)\n",
    "    anime_neg_sum = np.sum(\n",
    "        anime_embeddings[batch_anime_idx] * (10 - batch_anime_rating.reshape(-1, 1)), axis=0\n",
    "    ).astype(np.float32)\n",
    "    result_sum_rating =  anime_sum / sum_rating\n",
    "    result_mean_rating = anime_sum / num_anime_watched\n",
    "    result_sum_neg_rating = anime_neg_sum / sum_neg_rating\n",
    "    result = np.concatenate([\n",
    "        result_sum_rating,\n",
    "        result_sum_neg_rating,\n",
    "        result_mean_rating\n",
    "    ])\n",
    "    return result\n",
    "\n",
    "def extract_required_format(record, pca, mapping, user_embeddings, anime_embeddings, device):\n",
    "    return input_anime_embeddings(record, mapping, anime_embeddings), pca.transform([\n",
    "            user_embeddings[mapping['user2idx'][record['user_id']]]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "transform = partial(extract_required_format, pca=pca, mapping=mapping,\n",
    "                    user_embeddings=user_embeddings.cpu().detach().numpy(),\n",
    "                    anime_embeddings=anime_embeddings.cpu().detach().numpy(), device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeRatingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, sqlite_file, anime_embeddings, user_embeddings, mapping, pca, transform=None):\n",
    "        self.df = pd.read_pickle(sqlite_file).reset_index()\n",
    "        self.anime_embeddings = anime_embeddings.cpu().detach().numpy()\n",
    "        self.user_embeddings = user_embeddings.cpu().detach().numpy()\n",
    "        self.mapping = mapping\n",
    "        self.pca = pca\n",
    "        self.length = self.df.shape[0] - 1\n",
    "\n",
    "    def input_anime_embeddings(self, record):\n",
    "        num_records = len(record['anime_id'])\n",
    "        batch_anime_idx = np.array([self.mapping['anime2idx'][x] for x in record['anime_id']], dtype=np.int32)\n",
    "        batch_anime_rating = np.array(record['my_score'], dtype=np.int16)\n",
    "        num_anime_watched = len(batch_anime_idx)\n",
    "        sum_rating = np.sum(batch_anime_rating)\n",
    "        sum_rating = 1 if sum_rating == 0 else sum_rating\n",
    "        sum_neg_rating = np.sum(10 - batch_anime_rating)\n",
    "        sum_neg_rating = 1 if sum_neg_rating == 0 else sum_neg_rating\n",
    "        anime_sum = np.sum(\n",
    "            self.anime_embeddings[batch_anime_idx] * batch_anime_rating.reshape(-1, 1), axis=0\n",
    "        ).astype(np.float32)\n",
    "        anime_neg_sum = np.sum(\n",
    "            self.anime_embeddings[batch_anime_idx] * (10 - batch_anime_rating.reshape(-1, 1)), axis=0\n",
    "        ).astype(np.float32)\n",
    "        result_sum_rating =  anime_sum / sum_rating\n",
    "        result_mean_rating = anime_sum / num_anime_watched\n",
    "        result_sum_neg_rating = anime_neg_sum / sum_neg_rating\n",
    "        result = np.concatenate([\n",
    "            result_sum_rating,\n",
    "            result_sum_neg_rating,\n",
    "            result_mean_rating\n",
    "        ])\n",
    "        return result\n",
    "\n",
    "    def extract_required_format(self, record):\n",
    "        return self.input_anime_embeddings(record), self.pca.transform([\n",
    "                self.user_embeddings[self.mapping['user2idx'][record['user_id']]]\n",
    "            ]).astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = int(index)\n",
    "        return self.extract_required_format(self.df.iloc[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = ConcatDataset([\n",
    "    AnimeRatingsDataset(\n",
    "        f,\n",
    "        mapping=mapping,\n",
    "        anime_embeddings=anime_embeddings,\n",
    "        user_embeddings=user_embeddings,\n",
    "        pca=pca\n",
    "    ) for f in user_grouped_rating_files\n",
    "])\n",
    "\n",
    "train_size = int(len(total_dataset) * 0.8)\n",
    "test_size = int(len(total_dataset) * 0.2)\n",
    "total = sum([train_size, test_size])\n",
    "diff = len(total_dataset) - total\n",
    "train_dataset, test_dataset = random_split(total_dataset, (train_size + diff, test_size))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, anime_embedding_dim, batch_size=batch_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.anime_embedding_dim = anime_embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.fc1 = nn.Linear(3 * anime_embedding_dim, 100)\n",
    "        self.ln1 = nn.LayerNorm(100)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.ln2 = nn.LayerNorm(50)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Autoencoder encoder stage\n",
    "        l1_out = self.drop1(self.ln1(F.relu(self.fc1(x))))\n",
    "        l2_out = self.drop2(self.ln2(F.relu(self.fc2(l1_out))))\n",
    "        l3_out = self.fc3(l2_out)\n",
    "        return l3_out\n",
    "\n",
    "\n",
    "model = Net(anime_embedding_dim=50)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, optimizer, criterion):\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Running epoch {}'.format(epoch + 1))\n",
    "        train_epoch_loss = []\n",
    "        validation_epoch_loss = []\n",
    "        model = model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        # Model Training\n",
    "        for idx, (X, y) in enumerate(train_dataloader):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of anime indices.\n",
    "            #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "            prediction = model(X)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = criterion(prediction, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss.append(float(loss))\n",
    "            if idx % 40 == 0:\n",
    "                print('Batch {} - Training loss: {}'.format(idx + 1, loss))\n",
    "            del loss\n",
    "            del prediction\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model = model.eval()\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for idx, (X, y) in enumerate(test_dataloader):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                model.zero_grad()\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of anime indices.\n",
    "                #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "                prediction = model(X)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = criterion(prediction, y)\n",
    "                validation_epoch_loss.append(float(loss))\n",
    "                if idx % 10 == 0:\n",
    "                    print('Batch {} - Validation loss: {}'.format(idx + 1, loss))\n",
    "                del loss\n",
    "                del prediction\n",
    "            model = model.train()\n",
    "\n",
    "        train_loss.append(np.mean(train_epoch_loss))\n",
    "        validation_loss.append(np.mean(validation_epoch_loss))\n",
    "        print('Epoch {}: Mean training loss: {} Mean validation loss: {}'.format(epoch + 1, train_loss[-1], validation_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(num_epochs=15, optimizer=optimizer, model=model, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train(num_epochs=15, optimizer=optimizer, model=model, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "train(num_epochs=15, optimizer=optimizer, model=model, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neigh = NearestNeighbors(5, 0.4).fit(pca.transform(user_embeddings.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_users(anime_history, anime_ratings):\n",
    "    a = total_dataset.datasets[0]\n",
    "    with torch.no_grad():\n",
    "        prediction_model = model.eval()\n",
    "        for param in prediction_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        input_embeddings = a.input_anime_embeddings({\n",
    "            'anime_id': np.array(anime_history),\n",
    "            'my_score': np.array(anime_ratings)\n",
    "        })\n",
    "        predicted_embeddings = prediction_model(torch.from_numpy(input_embeddings).view(1, -1).to(device))\n",
    "        return neigh.kneighbors(predicted_embeddings.cpu().detach().numpy(), 2, return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id = [20, 21, 1, 121, 136]\n",
    "titles = ['Naruto', 'One Piece', 'Cowboy Bebop', 'Fullmetal Alchemist', 'Hunter x Hunter']\n",
    "ratings = [7.5, 7.2, 8, 8.2, 8.5]\n",
    "\n",
    "dist, closest_users = closest_users(anime_id, ratings)\n",
    "dist, closest_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
