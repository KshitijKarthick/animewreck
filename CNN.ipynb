{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 960M'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_anime = (108709, 6668)\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_grouped_rating_files = [f for f in glob('datasets/user_grouped_ratings/augmented_10_user_grouped_ratings_processed_*.db')]\n",
    "user_grouped_rating_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeRatingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, sqlite_file, transform=None):\n",
    "        self.db = sqlite3.connect(sqlite_file)\n",
    "        self.cursor = self.db.cursor()\n",
    "        self.length = self.cursor.execute('SELECT count(blob) from augmented_data;').fetchone()[0]\n",
    "\n",
    "    def extract_required_format(self, record):\n",
    "        record_df = pd.DataFrame({'anime_id': record['anime_id'], 'my_score': record['my_score']})\n",
    "        if len(record_df) > 5:\n",
    "            #num_of_seq = np.random.randint(5, len(record_df))\n",
    "            num_of_seq = 6\n",
    "            indexes = np.random.choice(record_df.index, size=num_of_seq)\n",
    "        else:\n",
    "            num_of_seq = np.random.randint(2, len(record_df))\n",
    "            indexes = np.random.choice(record_df.index, size=num_of_seq)\n",
    "        train = record_df.iloc[indexes[:-1]]\n",
    "        predict = record_df.iloc[indexes[-1:]]\n",
    "        X = np.concatenate([\n",
    "            [num_of_seq - 1],\n",
    "            train['anime_id'].values,\n",
    "            train['my_score'].values,\n",
    "            predict['anime_id'].values\n",
    "        ])\n",
    "        y = predict['my_score'].values\n",
    "        return X, y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = int(index)\n",
    "        row = self.cursor.execute('SELECT * from augmented_data where rowid=?', (index + 1, )).fetchone()\n",
    "        return self.extract_required_format(pickle.loads(row[1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = ConcatDataset([AnimeRatingsDataset(f) for f in user_grouped_rating_files[:2]])\n",
    "\n",
    "train_size = int(len(total_dataset) * 0.8)\n",
    "test_size = int(len(total_dataset) * 0.2)\n",
    "total = sum([train_size, test_size])\n",
    "diff = len(total_dataset) - total\n",
    "train_dataset, test_dataset = random_split(total_dataset, (train_size + diff, test_size))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 38)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (past_anime_embedding): Embedding(6668, 50)\n",
      "  (embedding_drop): Dropout(p=0.2)\n",
      "  (cv1): Conv1d(250, 64, kernel_size=(1,), stride=(1,))\n",
      "  (cv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "  (drop1): Dropout(p=0.2)\n",
      "  (mp1): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=183, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, anime_embedding_vocab, anime_embedding_dim,\n",
    "                 num_past_animes=5, batch_size=batch_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Store all the constants.\n",
    "        self.anime_embedding_vocab = anime_embedding_vocab\n",
    "        self.anime_embedding_dim = anime_embedding_dim\n",
    "        self.num_past_animes = num_past_animes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.past_anime_embedding = nn.Embedding(anime_embedding_vocab, anime_embedding_dim)\n",
    "        self.embedding_drop = nn.Dropout(0.2)\n",
    "\n",
    "        self.cv1 = nn.Conv1d(num_past_animes * anime_embedding_dim, 64, kernel_size=1)\n",
    "        self.cv2 = nn.Conv1d(64, 128, kernel_size=1)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.mp1 = nn.MaxPool1d(num_past_animes)\n",
    "        \n",
    "        # Previous history after max pooling, previous ratings, new history\n",
    "        self.fc1 = nn.Linear(128 + 50 + num_past_animes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        current_batch_size = x.shape[0]\n",
    "        num_past_records = 5\n",
    "        past_anime_historical_ids = x[:, 1: num_past_records + 1]\n",
    "        past_anime_ratings = x[:, num_past_records + 1:-1]\n",
    "        future_anime_id = x[:, -1:]\n",
    "\n",
    "        past_embeddings = self.embedding_drop(model.past_anime_embedding(past_anime_historical_ids))\n",
    "        future_embeddings = self.embedding_drop(model.past_anime_embedding(future_anime_id))\n",
    "\n",
    "        cv1 = self.cv1(past_embeddings.view(current_batch_size, -1, 1))\n",
    "        cv2 = self.drop1(model.cv2(cv1))\n",
    "\n",
    "        mp1 = self.mp1(cv2)\n",
    "\n",
    "        fc_in = torch.cat([\n",
    "            mp1.permute(1, 2, 0),\n",
    "            future_embeddings.view(current_batch_size, -1, 1).permute(1, 2, 0),\n",
    "            past_anime_ratings.view(current_batch_size, 5, 1).float().permute(1, 2, 0)\n",
    "        ]).permute(2, 0, 1).view(current_batch_size, -1)\n",
    "\n",
    "        return self.fc1(fc_in)\n",
    "\n",
    "\n",
    "model = Net(anime_embedding_dim=50, anime_embedding_vocab=num_anime)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1\n",
      "Batch 1 - Training loss: 9.66856575012207\n",
      "Batch 1 - Validation loss: 9.940427780151367\n",
      "Epoch 1: Mean training loss: 10.123922040587978 Mean validation loss: 10.070046324478952\n",
      "Running epoch 2\n",
      "Batch 1 - Training loss: 10.210956573486328\n",
      "Batch 1 - Validation loss: 9.966619491577148\n",
      "Epoch 2: Mean training loss: 10.121461378900628 Mean validation loss: 10.101415383188348\n",
      "Running epoch 3\n",
      "Batch 1 - Training loss: 10.469938278198242\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "validation_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    print('Running epoch {}'.format(epoch + 1))\n",
    "    train_epoch_loss = []\n",
    "    validation_epoch_loss = []\n",
    "    model = model.train()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Model Training\n",
    "    for idx, (X, y) in enumerate(train_dataloader):\n",
    "        current_batch_size = X.shape[0]\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of anime indices.\n",
    "        #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "        prediction = model(X)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(prediction, y.to(device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss.append(float(loss))\n",
    "        if idx % 200 == 0:\n",
    "            print('Batch {} - Training loss: {}'.format(idx + 1, loss))\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for idx, (X, y) in enumerate(test_dataloader):\n",
    "            current_batch_size = X.shape[0]\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of anime indices.\n",
    "            #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "            prediction = model(X)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = criterion(prediction, y.to(device).float())\n",
    "            validation_epoch_loss.append(float(loss))\n",
    "            if idx % 200 == 0:\n",
    "                print('Batch {} - Validation loss: {}'.format(idx + 1, loss))\n",
    "        model = model.train()\n",
    "\n",
    "    train_loss.append(np.mean(train_epoch_loss))\n",
    "    validation_loss.append(np.mean(validation_epoch_loss))\n",
    "    print('Epoch {}: Mean training loss: {} Mean validation loss: {}'.format(epoch + 1, train_loss[-1], validation_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict, 'CNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
