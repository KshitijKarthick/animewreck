{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user2idx', 'anime2idx'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = pickle.load(open('datasets/user_anime_ratings_mapping.pkl', 'rb'))\n",
    "mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108711, 6668)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users, num_anime = (len(mapping['user2idx']), len(mapping['anime2idx']))\n",
    "batch_size = 1024\n",
    "num_users, num_anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_grouped_rating_files = [f for f in glob('datasets/user_anime_ratings_db_split/user_anime_ratings_*.db')]\n",
    "user_grouped_rating_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeRatingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, sqlite_file, length=None, transform=None):\n",
    "        self.db = sqlite3.connect(sqlite_file)\n",
    "        self.cursor = self.db.cursor()\n",
    "        self.length = self.cursor.execute('SELECT count(user_id) from user_anime_ratings;').fetchone()[0] if length is None else length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = int(index)\n",
    "        _, user_id, anime_id, my_score = self.cursor.execute(\n",
    "            'SELECT * from user_anime_ratings where rowid=?', (index + 1, )).fetchone()\n",
    "        return np.array([\n",
    "            mapping['user2idx'][user_id], mapping['anime2idx'][anime_id]\n",
    "        ], dtype=np.long), np.array([my_score], dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "# class AnimeRatingsDataset(Dataset):\n",
    "#     \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "#     def __init__(self, sqlite_file, transform=None):\n",
    "#         self.data = h5py.File(sqlite_file, 'r')['user_anime_ratings']\n",
    "#         self.length = self.data['block0_values'].shape[0]\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         if isinstance(index, torch.Tensor):\n",
    "#             index = int(index)\n",
    "#         user_id, anime_id, my_score = self.data['block0_values'][index, :3]\n",
    "#         return np.array([\n",
    "#             mapping['user2idx'][user_id], mapping['anime2idx'][anime_id]\n",
    "#         ], dtype=np.long), np.array([my_score], dtype=np.float32)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = ConcatDataset([AnimeRatingsDataset(f) for f in user_grouped_rating_files])\n",
    "\n",
    "train_size = int(len(total_dataset) * 0.8)\n",
    "test_size = int(len(total_dataset) * 0.2)\n",
    "total = sum([train_size, test_size])\n",
    "diff = len(total_dataset) - total\n",
    "train_dataset, test_dataset = random_split(total_dataset, (train_size + diff, test_size))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24441, 6111)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (emb_user): Embedding(108711, 50)\n",
      "  (emb_anime): Embedding(6668, 50)\n",
      "  (ln1): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)\n",
      "  (drop1): Dropout(p=0.4)\n",
      "  (fc1): Linear(in_features=100, out_features=124, bias=True)\n",
      "  (ln2): LayerNorm(torch.Size([124]), eps=1e-05, elementwise_affine=True)\n",
      "  (drop2): Dropout(p=0.2)\n",
      "  (fc2): Linear(in_features=124, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_anime, num_users, anime_embedding_size,\n",
    "                 user_embedding_size, batch_size=batch_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.num_anime = num_anime\n",
    "        self.num_users = num_users\n",
    "        self.anime_embedding_size = anime_embedding_size\n",
    "        self.user_embedding_size = user_embedding_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.emb_user = nn.Embedding(num_users, user_embedding_size)\n",
    "        self.emb_anime = nn.Embedding(num_anime, anime_embedding_size)\n",
    "        self.ln1 = nn.LayerNorm(user_embedding_size + anime_embedding_size)\n",
    "        self.drop1 = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(user_embedding_size + anime_embedding_size, 124)\n",
    "        self.ln2= nn.LayerNorm(124)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(124, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_user_id, batch_anime_id = x[:, 0], x[:, 1]\n",
    "        anime_embeddings = self.emb_anime(batch_anime_id)\n",
    "        user_embeddings = self.emb_user(batch_user_id)\n",
    "        combined_embeddings = torch.cat([\n",
    "            anime_embeddings,\n",
    "            user_embeddings\n",
    "        ], dim=1)\n",
    "        fc1 = self.fc1(self.drop1(self.ln1(combined_embeddings)))\n",
    "        fc2 = self.fc2(self.drop2(self.ln2(fc1)))\n",
    "#         fc1 = self.fc1(self.drop1(combined_embeddings))\n",
    "#         fc2 = self.fc2(self.drop2(fc1))\n",
    "        return fc2\n",
    "\n",
    "\n",
    "model = Net(num_anime=num_anime, num_users=num_users,\n",
    "            user_embedding_size=50, anime_embedding_size=50)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, optimizer, criterion):\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Running epoch {}'.format(epoch + 1))\n",
    "        train_epoch_loss = []\n",
    "        validation_epoch_loss = []\n",
    "        model = model.train()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        # Model Training\n",
    "        for idx, (X, y) in enumerate(train_dataloader):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            X = X.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of anime indices.\n",
    "            #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "            prediction = model(X)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = criterion(prediction, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss.append(float(loss))\n",
    "            if idx % 12000 == 0:\n",
    "                print('Batch {} - Training loss: {}'.format(idx + 1, loss))\n",
    "            del loss\n",
    "            del prediction\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model = model.eval()\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for idx, (X, y) in enumerate(test_dataloader):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                model.zero_grad()\n",
    "\n",
    "                X = X.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of anime indices.\n",
    "                #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "                prediction = model(X)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = criterion(prediction, y)\n",
    "                validation_epoch_loss.append(float(loss))\n",
    "                if idx % 3000 == 0:\n",
    "                    print('Batch {} - Validation loss: {}'.format(idx + 1, loss))\n",
    "                del loss\n",
    "                del prediction\n",
    "            model = model.train()\n",
    "\n",
    "        train_loss.append(np.mean(train_epoch_loss))\n",
    "        validation_loss.append(np.mean(validation_epoch_loss))\n",
    "        print('Epoch {}: Mean training loss: {} Mean validation loss: {}'.format(epoch + 1, train_loss[-1], validation_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1\n",
      "Batch 1 - Training loss: 38.352134704589844\n",
      "Batch 12001 - Training loss: 11.515974044799805\n",
      "Batch 24001 - Training loss: 10.807840347290039\n",
      "Batch 1 - Validation loss: 10.270849227905273\n",
      "Batch 3001 - Validation loss: 10.612107276916504\n",
      "Batch 6001 - Validation loss: 10.6364107131958\n",
      "Epoch 1: Mean training loss: 11.810130416417804 Mean validation loss: 10.314588454678335\n",
      "Running epoch 2\n",
      "Batch 1 - Training loss: 10.771699905395508\n",
      "Batch 12001 - Training loss: 10.993139266967773\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(num_epochs=6, optimizer=optimizer, model=model, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, 'pytorch_nn_epoch6_embedding_fix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('pytorch_nn_epoch4_embedding_fix_10.23-10.21.pt')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train(num_epochs=10, optimizer=optimizer, model=model, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict, 'pytorch_nn_epoch25_embedding_fix.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict, 'pytorch_nn_9.885-9.8625.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.named_parameters()\n",
    "user_embeddings = next(params)[1]\n",
    "anime_embeddings = next(params)[1]\n",
    "user_embeddings.shape, anime_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(user_embeddings, open('user_embed_pytorch_nn_epoch4_embedding_fix_10.23-10.21.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(anime_embeddings, open('anime_embed_pytorch_nn_epoch4_embedding_fix_10.23-10.21.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['idx2user'] = {v:k for k, v in mapping['user2idx'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_user_embeddings = sorted(\n",
    "    [(mapping['idx2user'][idx], np.linalg.norm(x)) for idx, x in enumerate(user_embeddings.to('cpu').detach().numpy())], key=lambda x: x[1])\n",
    "(\"top min\", sorted_user_embeddings[:10]), (\"top max\", sorted_user_embeddings[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_anime_embeddings = sorted(\n",
    "    [(idx, np.linalg.norm(x)) for idx, x in enumerate(anime_embeddings.to('cpu').detach().numpy())], key=lambda x: x[1])\n",
    "(\"top min\", sorted_anime_embeddings[:10]), (\"top max\", sorted_anime_embeddings[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
