{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 960M'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_anime = (108709, 6668)\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_grouped_rating_files = [csv_file for csv_file in glob('datasets/user_grouped_ratings/user_grouped_ratings_processed*.pkl')]\n",
    "# user_grouped_rating_files.sort()\n",
    "\n",
    "user_grouped_rating_files = [f for f in glob('datasets/user_grouped_ratings/augmented_10_user_grouped_ratings_processed_*.db')]\n",
    "user_grouped_rating_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeRatingsDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, sqlite_file, transform=None):\n",
    "        self.db = sqlite3.connect(sqlite_file)\n",
    "        self.cursor = self.db.cursor()\n",
    "        self.length = self.cursor.execute('SELECT count(blob) from augmented_data;').fetchone()[0]\n",
    "\n",
    "    def extract_required_format(self, record):\n",
    "        record_df = pd.DataFrame({'anime_id': record['anime_id'], 'my_score': record['my_score']})\n",
    "        if len(record_df) > 5:\n",
    "            #num_of_seq = np.random.randint(5, len(record_df))\n",
    "            num_of_seq = 6\n",
    "            indexes = np.random.choice(record_df.index, size=num_of_seq)\n",
    "        else:\n",
    "            num_of_seq = np.random.randint(2, len(record_df))\n",
    "            indexes = np.random.choice(record_df.index, size=num_of_seq)\n",
    "        train = record_df.iloc[indexes[:-1]]\n",
    "        predict = record_df.iloc[indexes[-1:]]\n",
    "        X = np.concatenate([\n",
    "            [num_of_seq - 1],\n",
    "            train['anime_id'].values,\n",
    "            train['my_score'].values,\n",
    "            predict['anime_id'].values\n",
    "        ])\n",
    "        y = predict['my_score'].values\n",
    "        return X, y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, torch.Tensor):\n",
    "            index = int(index)\n",
    "        row = self.cursor.execute('SELECT * from augmented_data where rowid=?', (index + 1, )).fetchone()\n",
    "        return self.extract_required_format(pickle.loads(row[1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = ConcatDataset([AnimeRatingsDataset(f) for f in user_grouped_rating_files])\n",
    "\n",
    "train_size = int(len(total_dataset) * 0.8)\n",
    "test_size = int(len(total_dataset) * 0.2)\n",
    "total = sum([train_size, test_size])\n",
    "diff = len(total_dataset) - total\n",
    "train_dataset, test_dataset = random_split(total_dataset, (train_size + diff, test_size))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 189)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (past_anime_embedding): Embedding(6668, 50)\n",
      "  (embedding_drop): Dropout(p=0.2)\n",
      "  (lstm): LSTM(51, 256, bidirectional=True)\n",
      "  (drop1): Dropout(p=0.2)\n",
      "  (ln1): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=50, bias=True)\n",
      "  (ln2): LayerNorm(torch.Size([50]), eps=1e-05, elementwise_affine=True)\n",
      "  (drop2): Dropout(p=0.6)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, anime_embedding_vocab, anime_embedding_dim, lstm_hidden_dim,\n",
    "                 num_past_animes=5, batch_size=batch_size):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Store all the constants.\n",
    "        self.anime_embedding_vocab = anime_embedding_vocab\n",
    "        self.anime_embedding_dim = anime_embedding_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.num_past_animes = num_past_animes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.past_anime_embedding = nn.Embedding(anime_embedding_vocab, anime_embedding_dim)\n",
    "        self.embedding_drop = nn.Dropout(0.2)\n",
    "        # LSTM is fed the concatenated output of past anime ratings with their respective embeddings.\n",
    "        # It outputs the hidden state of size lstm_hidden_dim.\n",
    "        # anime embedding_size + 1 would suffice as anime_embedding_size is already * number of past records\n",
    "        self.lstm = nn.LSTM(anime_embedding_dim + 1, lstm_hidden_dim, bidirectional=True)\n",
    "\n",
    "        # Take the LSTM hidden state for the past anime watched with the future anime embedding\n",
    "        # as input to provide recommendation for the future anime.\n",
    "        # Final Hidden cells state, hidden state hence * 2\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        # Bidirectional hence * 2\n",
    "        self.ln1 = nn.LayerNorm((2 * lstm_hidden_dim * 2))\n",
    "        # Bidirectional hence * 2\n",
    "        self.fc1 = nn.Linear(lstm_hidden_dim * 2 * 2, self.anime_embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm((self.anime_embedding_dim))\n",
    "        # Historical embeddings + lstm past state\n",
    "        self.drop2 = nn.Dropout(0.6)\n",
    "        self.fc2 = nn.Linear(self.anime_embedding_dim + self.anime_embedding_dim, 1)\n",
    "        self.init_hidden(batch_size)\n",
    "\n",
    "    def init_hidden(self, minibatch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        self.hidden = (\n",
    "            torch.zeros(2, minibatch_size, self.lstm_hidden_dim).to(device),\n",
    "            torch.zeros(2, minibatch_size, self.lstm_hidden_dim).to(device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_past_records = 5\n",
    "        past_anime_historical_ids = x[:, 1: num_past_records + 1]\n",
    "        past_anime_ratings = x[:, num_past_records + 1:-1]\n",
    "        future_anime_id = x[:, -1:]\n",
    "\n",
    "        history_embeddings = self.past_anime_embedding(past_anime_historical_ids)\n",
    "        drop_history_embeddings = self.embedding_drop(history_embeddings)\n",
    "        future_embeddings = self.past_anime_embedding(future_anime_id)\n",
    "\n",
    "        lstm_input = torch.cat([\n",
    "            past_anime_ratings.view(-1, num_past_records, 1).permute(2, 1, 0).float(),\n",
    "            drop_history_embeddings.permute(2, 1, 0)\n",
    "        ]).permute(1, 2, 0)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            lstm_input,\n",
    "            self.hidden\n",
    "        )\n",
    "\n",
    "        final_hidden_concat_state = torch.cat([\n",
    "            self.hidden[0].permute(2, 1, 0),\n",
    "            self.hidden[1].permute(2, 1, 0)\n",
    "        ]).permute(1, 2, 0).contiguous().view(-1, self.lstm_hidden_dim * 2 * 2) # bidirectional hence * 2\n",
    "\n",
    "        dropout1 = self.drop1(final_hidden_concat_state)\n",
    "        ln1 = self.ln1(dropout1)\n",
    "        historical_state = F.relu(self.fc1(ln1))\n",
    "        ln2_historical_state = self.ln2(historical_state)        \n",
    "        recommendation_input = torch.cat([\n",
    "            future_embeddings.view(-1, self.anime_embedding_dim).permute(1, 0),\n",
    "            ln2_historical_state.permute(1, 0)\n",
    "        ]).permute(1, 0)\n",
    "\n",
    "        dropout2 = self.drop2(recommendation_input)\n",
    "        return self.fc2(dropout2)\n",
    "\n",
    "\n",
    "model = Net(anime_embedding_dim=50, anime_embedding_vocab=num_anime, lstm_hidden_dim=256)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1\n",
      "Batch 1 - Training loss: 10.55636978149414\n",
      "Batch 201 - Training loss: 10.310909271240234\n",
      "Batch 401 - Training loss: 10.577067375183105\n",
      "Batch 601 - Training loss: 10.77558708190918\n",
      "Batch 1 - Validation loss: 9.91697883605957\n",
      "Epoch 1: Mean training loss: 10.391021227710462 Mean validation loss: 9.992473693121047\n",
      "Running epoch 2\n",
      "Batch 1 - Training loss: 9.915519714355469\n",
      "Batch 201 - Training loss: 9.878466606140137\n",
      "Batch 401 - Training loss: 11.039697647094727\n",
      "Batch 601 - Training loss: 10.688434600830078\n",
      "Batch 1 - Validation loss: 8.883062362670898\n",
      "Epoch 2: Mean training loss: 10.39965880484808 Mean validation loss: 9.985361921724188\n",
      "Running epoch 3\n",
      "Batch 1 - Training loss: 10.542410850524902\n",
      "Batch 201 - Training loss: 10.662090301513672\n",
      "Batch 401 - Training loss: 10.596071243286133\n",
      "Batch 601 - Training loss: 10.600316047668457\n",
      "Batch 1 - Validation loss: 9.77141284942627\n",
      "Epoch 3: Mean training loss: 10.388995049491761 Mean validation loss: 9.974109770759704\n",
      "Running epoch 4\n",
      "Batch 1 - Training loss: 10.532787322998047\n",
      "Batch 201 - Training loss: 10.350408554077148\n",
      "Batch 401 - Training loss: 10.166851997375488\n",
      "Batch 601 - Training loss: 10.289339065551758\n",
      "Batch 1 - Validation loss: 9.952946662902832\n",
      "Epoch 4: Mean training loss: 10.394728933061872 Mean validation loss: 9.96390388125465\n",
      "Running epoch 5\n",
      "Batch 1 - Training loss: 10.284550666809082\n",
      "Batch 201 - Training loss: 10.603172302246094\n",
      "Batch 401 - Training loss: 10.436986923217773\n",
      "Batch 601 - Training loss: 9.90877628326416\n",
      "Batch 1 - Validation loss: 10.874167442321777\n",
      "Epoch 5: Mean training loss: 10.382350975874239 Mean validation loss: 10.018997394218646\n",
      "Running epoch 6\n",
      "Batch 1 - Training loss: 10.139152526855469\n",
      "Batch 201 - Training loss: 9.931550979614258\n",
      "Batch 401 - Training loss: 10.149206161499023\n",
      "Batch 601 - Training loss: 10.9856595993042\n",
      "Batch 1 - Validation loss: 9.852578163146973\n",
      "Epoch 6: Mean training loss: 10.36868456557945 Mean validation loss: 9.99496893907981\n",
      "Running epoch 7\n",
      "Batch 1 - Training loss: 10.884215354919434\n",
      "Batch 201 - Training loss: 10.537537574768066\n",
      "Batch 401 - Training loss: 9.818549156188965\n",
      "Batch 601 - Training loss: 10.028449058532715\n",
      "Batch 1 - Validation loss: 9.753473281860352\n",
      "Epoch 7: Mean training loss: 10.384881689434959 Mean validation loss: 9.982425437402473\n",
      "Running epoch 8\n",
      "Batch 1 - Training loss: 10.080570220947266\n",
      "Batch 201 - Training loss: 9.990318298339844\n",
      "Batch 401 - Training loss: 10.644169807434082\n",
      "Batch 601 - Training loss: 11.1874418258667\n",
      "Batch 1 - Validation loss: 9.800302505493164\n",
      "Epoch 8: Mean training loss: 10.394262254553498 Mean validation loss: 9.996666302756658\n",
      "Running epoch 9\n",
      "Batch 1 - Training loss: 10.84588623046875\n",
      "Batch 201 - Training loss: 10.152509689331055\n",
      "Batch 401 - Training loss: 10.43601131439209\n",
      "Batch 601 - Training loss: 10.462047576904297\n",
      "Batch 1 - Validation loss: 9.66663932800293\n",
      "Epoch 9: Mean training loss: 10.379551782809868 Mean validation loss: 9.97637620552507\n",
      "Running epoch 10\n",
      "Batch 1 - Training loss: 10.691595077514648\n",
      "Batch 201 - Training loss: 10.518533706665039\n",
      "Batch 401 - Training loss: 9.689323425292969\n",
      "Batch 601 - Training loss: 10.346030235290527\n",
      "Batch 1 - Validation loss: 10.293879508972168\n",
      "Epoch 10: Mean training loss: 10.383869513002022 Mean validation loss: 10.021028261336069\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "validation_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    print('Running epoch {}'.format(epoch + 1))\n",
    "    train_epoch_loss = []\n",
    "    validation_epoch_loss = []\n",
    "    model = model.train()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Model Training\n",
    "    for idx, (X, y) in enumerate(train_dataloader):\n",
    "        current_batch_size = X.shape[0]\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.init_hidden(minibatch_size=current_batch_size)\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of anime indices.\n",
    "        #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "        prediction = model(X)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = criterion(prediction, y.to(device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss.append(float(loss))\n",
    "        if idx % 200 == 0:\n",
    "            print('Batch {} - Training loss: {}'.format(idx + 1, loss))\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for idx, (X, y) in enumerate(test_dataloader):\n",
    "            current_batch_size = X.shape[0]\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.init_hidden(current_batch_size)\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of anime indices.\n",
    "            #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "            prediction = model(X)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = criterion(prediction, y.to(device).float())\n",
    "            validation_epoch_loss.append(float(loss))\n",
    "            if idx % 200 == 0:\n",
    "                print('Batch {} - Validation loss: {}'.format(idx + 1, loss))\n",
    "        model = model.train()\n",
    "\n",
    "    train_loss.append(np.mean(train_epoch_loss))\n",
    "    validation_loss.append(np.mean(validation_epoch_loss))\n",
    "    print('Epoch {}: Mean training loss: {} Mean validation loss: {}'.format(epoch + 1, train_loss[-1], validation_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(\n",
    "    model,\n",
    "    'augmented_files_{}_epochs_{}-{}.pt'.format(num_epochs, float(train_loss[-1]), float(validation_loss[-1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (past_anime_embedding): Embedding(6668, 50)\n",
       "  (embedding_drop): Dropout(p=0.2)\n",
       "  (lstm): LSTM(51, 256, bidirectional=True)\n",
       "  (drop1): Dropout(p=0.2)\n",
       "  (ln1): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=50, bias=True)\n",
       "  (ln2): LayerNorm(torch.Size([50]), eps=1e-05, elementwise_affine=True)\n",
       "  (drop2): Dropout(p=0.6)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torch.load('augmented_files_10_epochs_10.383869513002022-10.021028261336069.pt')\n",
    "# torch.save(model.state_dict(), 'augmented_files_10_epochs_10.383869513002022-10.021028261336069_state_dict.pt')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (past_anime_embedding): Embedding(6668, 50)\n",
       "  (embedding_drop): Dropout(p=0.2)\n",
       "  (lstm): LSTM(51, 256, bidirectional=True)\n",
       "  (drop1): Dropout(p=0.2)\n",
       "  (ln1): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=50, bias=True)\n",
       "  (ln2): LayerNorm(torch.Size([50]), eps=1e-05, elementwise_affine=True)\n",
       "  (drop2): Dropout(p=0.6)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torch.load('augmented_files_10_epochs_10.43818097013645-9.978318759373256.pt')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['idx2anime', 'user2idx', 'anime_titles', 'usernames2userid', 'idx2user', 'anime2idx'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "mapping = json.load(open('datasets/processed_ratings/anime_user_rating_mapping.json'))\n",
    "mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id = [20, 21, 1, 121, 136]\n",
    "titles = ['Naruto', 'One Piece', 'Cowboy Bebop', 'Fullmetal Alchemist', 'Hunter x Hunter']\n",
    "idx = [mapping['anime2idx'][str(x)] for x in anime_id]\n",
    "ratings = [7.5, 7.2, 8, 8.2, 8.5]\n",
    "predicted = [9] # bleach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6668/6668 [00:08<00:00, 788.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(73, 'Sen to Chihiro no Kamikakushi', 8.858256340026855),\n",
       " (105, 'Death Note', 8.656755447387695),\n",
       " (93, 'Howl no Ugoku Shiro', 8.480950355529785),\n",
       " (171, 'Shingeki no Kyojin', 8.381499290466309),\n",
       " (70, 'Mononoke Hime', 8.329761505126953),\n",
       " (196, 'One Punch Man', 8.30617618560791),\n",
       " (122, 'Code Geass: Hangyaku no Lelouch R2', 8.272485733032227),\n",
       " (202, 'Kimi no Na wa.', 8.199079513549805),\n",
       " (912, 'Tonari no Totoro', 8.078115463256836),\n",
       " (248, 'Final Fantasy VII: Advent Children', 8.040654182434082)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model = model.eval()\n",
    "for param in prediction_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def build_record(history_anime_id, history_ratings, new_anime):\n",
    "    return np.concatenate([\n",
    "        [5],\n",
    "        history_anime_id[:5],\n",
    "        history_ratings[:5],\n",
    "        [new_anime]\n",
    "    ])\n",
    "\n",
    "def anime_recommendations(history_anime_id, history_ratings, new_anime):\n",
    "    X = torch.from_numpy(np.array(\n",
    "        build_record(history_anime_id, history_ratings, new_anime)\n",
    "    ).reshape(1, -1)).to(device)\n",
    "    with torch.no_grad():\n",
    "        current_batch_size = X.shape[0]\n",
    "        prediction_model.zero_grad()\n",
    "        prediction_model.init_hidden(current_batch_size)\n",
    "        result = prediction_model(X)\n",
    "    return result\n",
    "\n",
    "def obtain_top_n(history_anime_id, history_ratings, topn=10):\n",
    "    watched = set(history_anime_id)\n",
    "    anime_ratings = []\n",
    "    for new_anime_idx in tqdm(range(num_anime)):\n",
    "        if new_anime_idx not in watched:\n",
    "            anime_ratings.append((\n",
    "                new_anime_idx,\n",
    "                float(anime_recommendations(history_anime_id, history_ratings, new_anime_idx)[0][0])\n",
    "            ))\n",
    "    top_anime_ratings = [\n",
    "        (anime_idx, mapping['anime_titles'][str(mapping['idx2anime'][str(anime_idx)])],rating)\n",
    "        for anime_idx, rating in sorted(anime_ratings, key=lambda x: x[1], reverse=True)[:topn]\n",
    "    ]\n",
    "    return top_anime_ratings\n",
    "    \n",
    "\n",
    "# anime_recommendations(\n",
    "#     history_anime_id=[56, 0, 53, 65, 708],\n",
    "#     history_ratings=[7, 7, 8, 8, 8],\n",
    "#     new_anime=162\n",
    "# )\n",
    "\n",
    "obtain_top_n(\n",
    "    history_anime_id=[56, 0, 53, 65, 708],\n",
    "    history_ratings=[7, 7, 8, 8, 8],\n",
    "    topn=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_list = pd.read_csv('datasets/anime_cleaned.csv')[['anime_id' ,'title', 'title_english']]\n",
    "# anime_list['id'] = anime_list['anime_id'].map(lambda x: mapping['anime2idx'][str(x)])\n",
    "\n",
    "# diff_names = anime_list[anime_list['title'] != anime_list['title_english']].dropna(subset=['title_english']).copy()\n",
    "# diff_names['title'] = diff_names['title_english']\n",
    "# result = pd.concat([anime_list, diff_names])\n",
    "\n",
    "# result.to_json('anime_list.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for X, y in train_dataloader:\n",
    "#     # Step 1. Remember that Pytorch accumulates gradients.\n",
    "#     # We need to clear them out before each instance\n",
    "#     model.zero_grad()\n",
    "\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "#     # Also, we need to clear out the hidden state of the LSTM,\n",
    "#     # detaching it from its history on the last instance.\n",
    "#     model.hidden = model.init_hidden()\n",
    "\n",
    "#     # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "#     # Tensors of anime indices.\n",
    "#     #record = torch.from_numpy(np.array([ 3, 23, 43, 53,  5,  4,  3, 67], dtype=np.int64)).to(device)\n",
    "\n",
    "#     prediction = model(X)\n",
    "\n",
    "#     # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "#     #  calling optimizer.step()\n",
    "#     loss = criterion(prediction, y.to(device).float())\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self, anime_embedding_vocab, anime_embedding_dim, lstm_hidden_dim,\n",
    "#                  num_past_animes=5, batch_size=batch_size):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         # Store all the constants.\n",
    "#         self.anime_embedding_vocab = anime_embedding_vocab\n",
    "#         self.anime_embedding_dim = anime_embedding_dim\n",
    "#         self.lstm_hidden_dim = lstm_hidden_dim\n",
    "#         self.num_past_animes = num_past_animes\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#         self.past_anime_embedding = nn.Embedding(anime_embedding_vocab, anime_embedding_dim)\n",
    "#         self.embedding_drop = nn.Dropout(0.2)\n",
    "#         # LSTM is fed the concatenated output of past anime ratings with their respective embeddings.\n",
    "#         # It outputs the hidden state of size lstm_hidden_dim.\n",
    "#         # anime embedding_size + 1 would suffice as anime_embedding_size is already * number of past records\n",
    "#         self.lstm = nn.LSTM(anime_embedding_dim + 1, lstm_hidden_dim, bidirectional=False)\n",
    "\n",
    "#         # Take the LSTM hidden state for the past anime watched with the future anime embedding\n",
    "#         # as input to provide recommendation for the future anime.\n",
    "#         # Final Hidden cells state, hidden state hence * 2\n",
    "#         self.drop1 = nn.Dropout(0.2)\n",
    "#         self.ln1 = nn.LayerNorm((2 * lstm_hidden_dim))\n",
    "#         self.fc1 = nn.Linear(lstm_hidden_dim * 2, self.anime_embedding_dim)\n",
    "#         self.ln2 = nn.LayerNorm((self.anime_embedding_dim))\n",
    "#         # Historical embeddings + lstm past state\n",
    "#         self.drop2 = nn.Dropout(0.6)\n",
    "#         self.fc2 = nn.Linear(self.anime_embedding_dim + self.anime_embedding_dim, 1)\n",
    "#         self.init_hidden(batch_size)\n",
    "\n",
    "#     def init_hidden(self, minibatch_size):\n",
    "#         # Before we've done anything, we dont have any hidden state.\n",
    "#         # Refer to the Pytorch documentation to see exactly\n",
    "#         # why they have this dimensionality.\n",
    "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "#         self.hidden = (\n",
    "#             torch.zeros(1, minibatch_size, self.lstm_hidden_dim).to(device),\n",
    "#             torch.zeros(1, minibatch_size, self.lstm_hidden_dim).to(device)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         num_past_records = 5\n",
    "#         past_anime_historical_ids = x[:, 1: num_past_records + 1]\n",
    "#         past_anime_ratings = x[:, num_past_records + 1:-1]\n",
    "#         future_anime_id = x[:, -1:]\n",
    "\n",
    "#         history_embeddings = self.past_anime_embedding(past_anime_historical_ids)\n",
    "#         drop_history_embeddings = self.embedding_drop(history_embeddings)\n",
    "#         future_embeddings = self.past_anime_embedding(future_anime_id)\n",
    "\n",
    "#         lstm_input = torch.cat([\n",
    "#             past_anime_ratings.view(-1, num_past_records, 1).permute(2, 1, 0).float(),\n",
    "#             drop_history_embeddings.permute(2, 1, 0)\n",
    "#         ]).permute(1, 2, 0)\n",
    "\n",
    "#         lstm_out, self.hidden = self.lstm(\n",
    "#             lstm_input,\n",
    "#             self.hidden\n",
    "#         )\n",
    "\n",
    "#         final_hidden_concat_state = torch.cat([\n",
    "#             self.hidden[0].view(-1, self.lstm_hidden_dim).permute(1, 0),\n",
    "#             self.hidden[1].view(-1, self.lstm_hidden_dim).permute(1, 0)\n",
    "#         ]).permute(1, 0)\n",
    "\n",
    "#         dropout1 = self.drop1(final_hidden_concat_state)\n",
    "#         ln1 = self.ln1(dropout1)\n",
    "#         historical_state = F.relu(self.fc1(ln1))\n",
    "#         ln2_historical_state = self.ln2(historical_state)        \n",
    "#         recommendation_input = torch.cat([\n",
    "#             future_embeddings.view(-1, self.anime_embedding_dim).permute(1, 0),\n",
    "#             ln2_historical_state.permute(1, 0)\n",
    "#         ]).permute(1, 0)\n",
    "\n",
    "#         dropout2 = self.drop2(recommendation_input)\n",
    "#         return self.fc2(dropout2)\n",
    "\n",
    "\n",
    "# model = Net(anime_embedding_dim=50, anime_embedding_vocab=num_anime, lstm_hidden_dim=256)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self, anime_embedding_vocab, anime_embedding_dim, lstm_hidden_dim, num_past_animes=5):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         # Store all the constants.\n",
    "#         self.anime_embedding_vocab = anime_embedding_vocab\n",
    "#         self.anime_embedding_dim = anime_embedding_dim\n",
    "#         self.lstm_hidden_dim = lstm_hidden_dim\n",
    "#         self.num_past_animes = num_past_animes\n",
    "\n",
    "#         # Take (num_past_animes) embedding inputs to learn the relation on how to predict\n",
    "#         # Anime, given past history.\n",
    "# #         self.past_anime_embeddings = []\n",
    "# #         for _ in range(num_past_animes):\n",
    "# #             self.past_anime_embeddings.append(nn.Embedding(anime_vocab, anime_embedding_dim))\n",
    "#         self.past_anime_embedding = nn.Embedding(anime_embedding_vocab, anime_embedding_dim)\n",
    "\n",
    "# #         # Embedding for the future anime which needs the rating to be predicted.\n",
    "# #         self.anime_future_embedding = nn.Embedding(anime_embedding_vocab, anime_embedding_dim)\n",
    "\n",
    "#         # Total embedding size for anime history.\n",
    "# #         total_anime_embedding_size = num_past_animes * anime_embedding_dim\n",
    "#         total_anime_embedding_size = anime_embedding_dim\n",
    "\n",
    "#         # Total size for storing the ratings for past anime watched.\n",
    "#         total_anime_rating_size = 1\n",
    "\n",
    "#         # LSTM is fed the concatenated output of past anime ratings with their respective embeddings.\n",
    "#         # It outputs the hidden state of size lstm_hidden_dim.\n",
    "#         self.lstm = nn.LSTM(total_anime_embedding_size + total_anime_rating_size, lstm_hidden_dim)\n",
    "\n",
    "#         # Take the LSTM hidden state for the past anime watched with the future anime embedding\n",
    "#         # as input to provide recommendation for the future anime.\n",
    "#         # Final Hidden cells state, hidden state hence * 2\n",
    "#         self.fc1 = nn.Linear(lstm_hidden_dim * 2, self.anime_embedding_dim)\n",
    "#         # Historical embeddings + lstm past state\n",
    "#         self.fc2 = nn.Linear(self.anime_embedding_dim + self.anime_embedding_dim, 1)\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         # Before we've done anything, we dont have any hidden state.\n",
    "#         # Refer to the Pytorch documentation to see exactly\n",
    "#         # why they have this dimensionality.\n",
    "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "#         return (torch.zeros(1, 1, self.lstm_hidden_dim).to(device),\n",
    "#                 torch.zeros(1, 1, self.lstm_hidden_dim).to(device))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         num_past_records = int(x[0])\n",
    "#         past_anime_historical_ids = x[1: num_past_records + 1]\n",
    "#         past_anime_ratings = x[num_past_records + 1:-1]\n",
    "#         future_anime_id = x[-1:]\n",
    "# #         embedding_outputs = []\n",
    "# #         for i in range(self.num_past_animes):\n",
    "# #             if num_past_records < i:\n",
    "# #                 out = past_anime_embeddings[i](past_anime_historical_ids[i])\n",
    "# #             else:\n",
    "# #                 # The current record does not have as much as history\n",
    "# #                 out = torch.zeros(1, self.anime_embedding_dim)\n",
    "# #             embedding_outputs.append(out)\n",
    "# #         total_past_anime_embeddings = torch.cat(embedding_outputs)\n",
    "\n",
    "#         history_embeddings = self.past_anime_embedding(past_anime_historical_ids)\n",
    "#         future_embeddings = self.past_anime_embedding(future_anime_id)\n",
    "\n",
    "#         lstm_input = torch.cat([\n",
    "#             past_anime_ratings.view(1, num_past_records).float(),\n",
    "#             history_embeddings.permute(1, 0)\n",
    "#         ]).permute(1, 0)\n",
    "\n",
    "#         lstm_out, self.hidden = self.lstm(\n",
    "#             lstm_input.view(num_past_records, 1, -1),\n",
    "#             self.hidden\n",
    "#         )\n",
    "        \n",
    "#         final_hidden_concat_state = torch.cat([\n",
    "#             self.hidden[0].view(self.lstm_hidden_dim),\n",
    "#             self.hidden[1].view(self.lstm_hidden_dim)\n",
    "#         ])\n",
    "\n",
    "#         historical_state = F.relu(self.fc1(final_hidden_concat_state))\n",
    "\n",
    "# #         set_trace()\n",
    "        \n",
    "#         recommendation_input = torch.cat([\n",
    "#             future_embeddings.view(self.anime_embedding_dim),\n",
    "#             historical_state\n",
    "#         ])\n",
    "\n",
    "#         return self.fc2(recommendation_input)\n",
    "\n",
    "\n",
    "# model = Net(anime_embedding_dim=50, anime_embedding_vocab=num_anime, lstm_hidden_dim=256)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self, anime_embedding_vocab, anime_embedding_dim, lstm_hidden_dim, num_past_animes=5):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         # Store all the constants.\n",
    "#         self.anime_embedding_vocab = anime_embedding_vocab\n",
    "#         self.anime_embedding_dim = anime_embedding_dim\n",
    "#         self.lstm_hidden_dim = lstm_hidden_dim\n",
    "#         self.num_past_animes = num_past_animes\n",
    "\n",
    "#         self.past_anime_embedding = nn.Embedding(anime_embedding_vocab, anime_embedding_dim)\n",
    "\n",
    "#         # LSTM is fed the concatenated output of past anime ratings with their respective embeddings.\n",
    "#         # It outputs the hidden state of size lstm_hidden_dim.\n",
    "#         # anime embedding_size + 1 would suffice as anime_embedding_size is already * number of past records\n",
    "#         self.lstm = nn.LSTM(anime_embedding_dim + 1, lstm_hidden_dim)\n",
    "\n",
    "#         # Take the LSTM hidden state for the past anime watched with the future anime embedding\n",
    "#         # as input to provide recommendation for the future anime.\n",
    "#         # Final Hidden cells state, hidden state hence * 2\n",
    "#         self.fc1 = nn.Linear(lstm_hidden_dim * 2, self.anime_embedding_dim)\n",
    "\n",
    "#         # Historical embeddings + lstm past state\n",
    "#         self.fc2 = nn.Linear(self.anime_embedding_dim + self.anime_embedding_dim, 1)\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         # Before we've done anything, we dont have any hidden state.\n",
    "#         # Refer to the Pytorch documentation to see exactly\n",
    "#         # why they have this dimensionality.\n",
    "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "#         return (torch.zeros(1, 1, self.lstm_hidden_dim).to(device),\n",
    "#                 torch.zeros(1, 1, self.lstm_hidden_dim).to(device))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         num_past_records = int(x[0])\n",
    "#         past_anime_historical_ids = x[1: num_past_records + 1]\n",
    "#         past_anime_ratings = x[num_past_records + 1:-1]\n",
    "#         future_anime_id = x[-1:]\n",
    "\n",
    "#         history_embeddings = self.past_anime_embedding(past_anime_historical_ids)\n",
    "#         future_embeddings = self.past_anime_embedding(future_anime_id)\n",
    "\n",
    "#         lstm_input = torch.cat([\n",
    "#             past_anime_ratings.view(1, num_past_records).float(),\n",
    "#             history_embeddings.permute(1, 0)\n",
    "#         ]).permute(1, 0)\n",
    "\n",
    "#         lstm_out, self.hidden = self.lstm(\n",
    "#             lstm_input.view(num_past_records, 1, -1),\n",
    "#             self.hidden\n",
    "#         )\n",
    "        \n",
    "#         final_hidden_concat_state = torch.cat([\n",
    "#             self.hidden[0].view(self.lstm_hidden_dim),\n",
    "#             self.hidden[1].view(self.lstm_hidden_dim)\n",
    "#         ])\n",
    "\n",
    "#         historical_state = F.relu(self.fc1(final_hidden_concat_state))\n",
    "        \n",
    "#         recommendation_input = torch.cat([\n",
    "#             future_embeddings.view(self.anime_embedding_dim),\n",
    "#             historical_state\n",
    "#         ])\n",
    "\n",
    "#         return self.fc2(recommendation_input)\n",
    "\n",
    "\n",
    "# model = Net(anime_embedding_dim=50, anime_embedding_vocab=num_anime, lstm_hidden_dim=256)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
